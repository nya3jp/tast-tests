{
  "comments": [
    {
      "key": {
        "uuid": "14bb632d_b1606ffa",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 7
      },
      "lineNbr": 0,
      "author": {
        "id": 1113505
      },
      "writtenOn": "2020-07-03T08:39:11Z",
      "side": 1,
      "message": "\u003e Patch Set 7:\n\u003e \n\u003e Hmm. \"Failed to wait wiping finished: wipe_init failed\" error is still concerning if we haven\u0027t done anything about it.\n\u003e \n\u003e Wild guess: is it possible that it\u0027s too early after reboot to call gooftool? wipe_init.log says:\n\u003e \n\u003e  [2020-06-25 00:52:02,464] INFO:root:Running services (service_name, LOG_PATH): [(\u0027boot-services\u0027, \u0027\u0027), (\u0027failsafe\u0027, \u0027\u0027), (\u0027dbus\u0027, \u0027\u0027), (\u0027wpasupplicant\u0027, \u0027\u0027), (\u0027openssh-server\u0027, \u0027\u0027), (\u0027shill\u0027, \u0027\u0027), (\u0027sslh\u0027, \u0027\u0027)]\n\u003e  [2020-06-25 00:52:02,465] INFO:root:Going to stop services (service_name, LOG_PATH): []\n\u003e \n\u003e The \"going to stop\" list is empty, and I guess it\u0027s not what you expect (e.g. \"ui\" will be there usually). It seems to suggest that the system is still booting.\n\nOur code will scan and stop the daemons 3 times to prevent that some daemons are hanged and not stopped. The log you list is for the 2nd and the 3rd times. From much earlier log, you can see the list is not empty and we did the killing.\n\n\u003e Wild guess: is it possible that it\u0027s too early after reboot to call gooftool?\n\nI also worried about that, but I haven\u0027t seen any failure caused by killing process too early.\nI can add a sleep() to waiting for a while after boot. (Any suggestion about how long we should wait?) But it may not help for the issue you mentioned. I have tried to catch this by add more log and local run 1000 times, but no luck. It only happened for a week on Tast dashboard and then disappeared.\n\nBTW, I have more concern about the DUT drop the connection sometimes. Do you have seem similar issue in other test?\n",
      "revId": "78fc07f0d306cbe1429ce28856e1ec690ee4f34d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "3ee6f3b8_117c54e3",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 7
      },
      "lineNbr": 0,
      "author": {
        "id": 1118346
      },
      "writtenOn": "2020-07-06T08:24:54Z",
      "side": 1,
      "message": "I looked at failures further.\n\n\u003e Dump wipe_in_tmpfs.log (before pivot root) fail\n\nLooking at syslog on this failure, SSH login is not attempted until the polling timeout is almost reached! I suspect that this is because d.Command(...).Run(ctx) can block up to ctx timeout.\n\nCan you try setting a per-iteration timeout in the following way?\n\n if err :\u003d testing.Poll(ctx, func(ctx context.Context) error {\n   ctx, cancel :\u003d context.WithTimeout(ctx, 10*time.Second)\n   defer cancel()\n\n   ...\n }...\n\nc.f.\nhttps://source.corp.google.com/chromeos_public/src/platform/tast/src/chromiumos/tast/dut/dut.go;l\u003d189\n\n\u003e Failed to wait wiping finished: wipe_init failed\n\nTast has some logic to wait for the DUT to get ready in the ready package. It is currently used only in pre-run hooks, but it should be useful for tests rebooting the DUT as well. ready.Wait function contains some pre-run-specific logics as well, so you might have to refactor it a bit.\n\n\u003e Failed to reboot DUT\n\nThis seems general boot flakiness. It is known that a DUT might not return from a reboot for some time due to boot flakiness, e.g. crbug.com/1040286#c38\n\n\u003e Failed to reconnect to DUT\n\nLooking at logs, they seem caused by crash.KernelCrash. It\u0027s tracked by crbug.com/1040286, so you can ignore those failures on evaluating your test.",
      "parentUuid": "14bb632d_b1606ffa",
      "revId": "78fc07f0d306cbe1429ce28856e1ec690ee4f34d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "db5129b0_5266d7bb",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 7
      },
      "lineNbr": 0,
      "author": {
        "id": 1113505
      },
      "writtenOn": "2020-07-06T11:22:38Z",
      "side": 1,
      "message": "\u003e Looking at syslog on this failure, SSH login is not attempted until the polling timeout is almost reached! I suspect that this is because d.Command(...).Run(ctx) can block up to ctx timeout.\n\nBut we already shorted the timeout at the beginning of the test. The cleanup() should at least have 1 minute to dump log and reboot.\n\nI prepared a CL for per-iteration timeout at CL:2282603, but I doubt it could fix the issue.\n\nFrom the log, it keeps showing\n\u003e  failed to create session: read tcp 10.0.195.119:38338-\u003e100.115.194.198:22: read: connection reset by peer\nDoesn\u0027t it means the somehow the DUT is on bad state and keep refusing the connection?\n\n\n\u003e \u003e Failed to wait wiping finished: wipe_init failed\n\u003e Tast has some logic to wait for the DUT to get ready in the ready package. It is currently used only in pre-run hooks, but it should be useful for tests rebooting the DUT as well. ready.Wait function contains some pre-run-specific logics as well, so you might have to refactor it a bit.\n\nIt seems ready.Wait() is for local test. It looks there are a lot of libs (upstart/dbus/...) that have to port. Can I re-implement similar logic inside this test? Like CL:2282604. In this case, I think we only need to wait system-service up.",
      "parentUuid": "3ee6f3b8_117c54e3",
      "revId": "78fc07f0d306cbe1429ce28856e1ec690ee4f34d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "57678477_3b0138b5",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 7
      },
      "lineNbr": 0,
      "author": {
        "id": 1118346
      },
      "writtenOn": "2020-07-07T09:56:26Z",
      "side": 1,
      "message": "\u003e \u003e Looking at syslog on this failure, SSH login is not attempted until the polling timeout is almost reached! I suspect that this is because d.Command(...).Run(ctx) can block up to ctx timeout.\n\u003e \n\u003e But we already shorted the timeout at the beginning of the test. The cleanup() should at least have 1 minute to dump log and reboot.\n\u003e \n\u003e I prepared a CL for per-iteration timeout at CL:2282603, but I doubt it could fix the issue.\n\u003e \n\u003e From the log, it keeps showing\n\u003e \u003e  failed to create session: read tcp 10.0.195.119:38338-\u003e100.115.194.198:22: read: connection reset by peer\n\u003e Doesn\u0027t it means the somehow the DUT is on bad state and keep refusing the connection?\n\nMy guess is that the following scenario might be happening:\n\n1. d.Command(\"gooftool\") is called\n2. testing.Poll is called\n3. d.Command(\"cat\") is called, but it fails with \"...: connection reset by peer\" because the device is going down\n4. the DUT reboots\n5. d.Command(\"cat\") is called again, but it doesn\u0027t fail until 1-minute timeout is reached because SYN packets to the DUT go to void\n6. testing.Poll returns with an error \"context deadline exceeded; last error follows: fail to access log: ...: connection reset by peer\" because \"connection reset by peer\" was the last error observed in (3).\n\n\n\u003e \u003e \u003e Failed to wait wiping finished: wipe_init failed\n\u003e \u003e Tast has some logic to wait for the DUT to get ready in the ready package. It is currently used only in pre-run hooks, but it should be useful for tests rebooting the DUT as well. ready.Wait function contains some pre-run-specific logics as well, so you might have to refactor it a bit.\n\u003e \n\u003e It seems ready.Wait() is for local test. It looks there are a lot of libs (upstart/dbus/...) that have to port. Can I re-implement similar logic inside this test? Like CL:2282604. In this case, I think we only need to wait system-service up.\n\nThat sounds good to me, thanks!",
      "parentUuid": "db5129b0_5266d7bb",
      "revId": "78fc07f0d306cbe1429ce28856e1ec690ee4f34d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    }
  ]
}