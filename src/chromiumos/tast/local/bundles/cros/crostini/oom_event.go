// Copyright 2022 The Chromium OS Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

package crostini

import (
	"context"
	"io/ioutil"
	"regexp"
	"time"

	"github.com/godbus/dbus/v5"

	"chromiumos/tast/common/testexec"
	"chromiumos/tast/ctxutil"
	"chromiumos/tast/errors"
	"chromiumos/tast/local/chrome"
	"chromiumos/tast/local/chrome/metrics"
	"chromiumos/tast/local/crash"
	"chromiumos/tast/local/crostini"
	"chromiumos/tast/local/dbusutil"
	"chromiumos/tast/local/vm"
	"chromiumos/tast/testing"
)

const (
	oomAnomalyEventServiceName        = "org.chromium.AnomalyEventService"
	oomAnomalyEventServicePath        = dbus.ObjectPath("/org/chromium/AnomalyEventService")
	oomAnomalyEventServiceInterface   = "org.chromium.AnomalyEventServiceInterface"
	oomAnomalyGuestOOMEventSignalName = "GuestOomEvent"
	crosEventHistogram                = "Platform.CrOSEvent"
	sigRegexp                         = "sig=guest-oom-event"
	logRegexp                         = ".* Out of memory: Killed process .*tail.*"
	oomEventHistogramEnum             = 34
	killCode                          = 9
)

func init() {
	testing.AddTest(&testing.Test{
		Func:         OOMEvent,
		LacrosStatus: testing.LacrosVariantUnneeded,
		Desc:         "Check that OOM kill by kernel is detected correctly",
		Contacts: []string{
			// Crosvm
			"drmasquatch@google.com",
			// Telemetry
			"mutexlox@google.com",
			"cros-telemetry@google.com",
		},
		SoftwareDeps: []string{"chrome", "vm_host"},
		Attr:         []string{"group:mainline"},
		Params: []testing.Param{
			// Parameters generated by params_test.go. DO NOT EDIT.
			{
				Name:              "stable",
				ExtraSoftwareDeps: []string{"dlc"},
				ExtraHardwareDeps: crostini.CrostiniStable,
				Fixture:           "crostiniBuster",
				Timeout:           10 * time.Minute,
			}, {
				Name:              "unstable",
				ExtraAttr:         []string{"informational"},
				ExtraSoftwareDeps: []string{"dlc"},
				ExtraHardwareDeps: crostini.CrostiniUnstable,
				Fixture:           "crostiniBuster",
				Timeout:           10 * time.Minute,
			},
		},
	})
}

// OOMEvent sets up the VM and then runs a process which will be killed via
// the kernels OOM killer to check that it is detected correctly.
func OOMEvent(ctx context.Context, s *testing.State) {
	pre := s.FixtValue().(crostini.FixtureData)
	cont := pre.Cont
	tconn := pre.Tconn

	// Use a shortened context for the test to reserver time for cleanup.
	cleanupCtx := ctx
	ctx, cancel := ctxutil.Shorten(ctx, 5*time.Second)
	defer cancel()

	s.Log("Setting up crash test")
	if err := crash.SetUpCrashTest(ctx, crash.WithMockConsent()); err != nil {
		s.Fatal("Failed to set up crash test: ", err)
	}
	defer func() {
		if err := crash.TearDownCrashTest(cleanupCtx); err != nil {
			s.Error("Failed to tear down crash test fixture: ", err)
		}
	}()

	s.Log("Getting baseline metrics histogram")
	histogram, err := metrics.GetHistogram(ctx, tconn, crosEventHistogram)
	if err != nil {
		s.Fatal("Failed to get baseline for histogram: ", err)
	}
	for _, bucket := range histogram.Buckets {
		if bucket.Min == oomEventHistogramEnum {
			s.Fatal("Histogram should not contain any OOM events yet")
		}
	}

	if err := checkDbusSignal(ctx, cont, s); err != nil {
		s.Fatal("Didn't get an error signal for OOM process: ", err)
	}

	if err := checkOOMHistogram(ctx, tconn, histogram); err != nil {
		s.Fatal("Could not get updated histogram: ", err)
	}

	if err := checkCrashReport(ctx, s); err != nil {
		s.Fatal("Could not find crash report: ", err)
	}
}

func checkCrashReport(ctx context.Context, s *testing.State) error {
	s.Log("Checking for expected crash reports")

	daemonStorePaths, err := crash.GetDaemonStoreCrashDirs(ctx)
	if err != nil {
		s.Fatal("Failed to get daemon store crash dir: ", err)
	}

	files, err := crash.WaitForCrashFiles(ctx, daemonStorePaths,
		[]string{`guest_oom_event.*\.meta`,
			`guest_oom_event.*\.log`})
	if err != nil {
		s.Fatal("Couldn't find expected files: ", err)
	}

	s.Log("Checking for expected metadata signature")

	metaData, err := ioutil.ReadFile(files[`guest_oom_event.*\.meta`][0])
	if err != nil {
		s.Fatal("Failed to read the metadata file: ", err)
	}
	if re := regexp.MustCompile(sigRegexp); !re.Match(metaData) {
		s.Fatalf("Did not find expected line %q in metadata file", sigRegexp)
	}

	log, err := ioutil.ReadFile(files[`guest_oom_event.*\.log`][0])
	if err != nil {
		s.Fatal("Couldn't read log file: ", err)
	}
	if re := regexp.MustCompile(logRegexp); !re.Match(log) {
		s.Fatalf("Did not find expected line %q in log file", logRegexp)
	}

	// If the crash report files were as expected, delete
	// them. This stops them from being uploaded to the crash
	// server and polluting the data with fake crashes.
	//
	// Don't die on error, because this is just a cleanup step.
	if err = crash.RemoveAllFiles(ctx, files); err != nil {
		s.Log("Failed to clean up generated crash files: ", err)
	}

	return nil
}

func checkOOMHistogram(ctx context.Context, tconn *chrome.TestConn, histogram *metrics.Histogram) error {
	testing.ContextLog(ctx, "Waiting for histogram update with OOM enum")
	err := testing.Poll(ctx, func(ctx context.Context) error {
		newHistogram, err := metrics.GetHistogram(ctx, tconn, crosEventHistogram)
		if err != nil {
			return testing.PollBreak(errors.Wrap(err, "failed to get new value of histogram"))
		}

		diff, err := newHistogram.Diff(histogram)
		if err != nil {
			return testing.PollBreak(errors.Wrap(err, "failed to diff histograms"))
		}

		for _, bucket := range diff.Buckets {
			if bucket.Min == oomEventHistogramEnum {
				// Got an oom event, returning nil signals testing.Poll to finish.
				return nil
			}
		}

		return errors.New("did not find Crostini.OomEvent in " + diff.String())
	}, nil)

	if err != nil {
		return errors.Wrap(err, "failed polling on metrics.GetHistogram")
	}

	testing.ContextLog(ctx, "Found the expected histogram")

	return nil
}

func checkDbusSignal(ctx context.Context, container *vm.Container, s *testing.State) (resultError error) {
	match := dbusutil.MatchSpec{
		Type:      "signal",
		Path:      oomAnomalyEventServicePath,
		Interface: oomAnomalyEventServiceInterface,
		Member:    oomAnomalyGuestOOMEventSignalName,
	}
	signalWatcher, err := dbusutil.NewSignalWatcherForSystemBus(ctx, match)
	if err != nil {
		return errors.Wrap(err, "failed to listen for DBus signals")
	}
	defer func() {
		if err := signalWatcher.Close(ctx); err != nil {
			if resultError == nil {
				resultError = err
			} else {
				testing.ContextLog(ctx, "Failed to close signal watcher: ", err)
			}
		}
	}()

	s.Log("Starting tail process")
	// tail will buffer input in-memory until it reaches a newline, so it can
	// print at least one line at a time. Since /dev/zero by definition has no
	// newlines, the memory of the process should expand until eventually the
	// kernel will be forced to kill it.
	cmd := container.VM.Command(ctx, "tail", "/dev/zero")
	code, extracted := testexec.ExitCode(cmd.Run())
	if !extracted {
		s.Fatal("Failed to extract exit code from running tail ")
	}
	if code != killCode {
		s.Fatalf("Failed to fail correctly, expected process to exit with code %v, exited with %v instead", killCode, code)
	}

	testing.ContextLog(ctx, "Waiting for signal from anomaly_detector")
	signalCtx, cancel := context.WithTimeout(ctx, 5*time.Second)
	defer cancel()
	if _, err := waitForAnomalyDetectorSignal(signalCtx, signalWatcher); err != nil {
		return errors.Wrap(err, "didn't get expected DBus signal")
	}
	testing.ContextLog(ctx, "Got expected signal from anomaly_detector")

	return nil
}

func waitForAnomalyDetectorSignal(ctx context.Context, signalWatcher *dbusutil.SignalWatcher) (*dbus.Signal, error) {
	select {
	case signal := <-signalWatcher.Signals:
		return signal, nil
	case <-ctx.Done():
		return nil, errors.New("Context deadline expired")
	}
}
