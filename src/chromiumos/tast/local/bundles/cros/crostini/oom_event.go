// Copyright 2022 The Chromium OS Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

package crostini

import (
	"context"
	"time"

	"github.com/godbus/dbus"

	"chromiumos/tast/common/testexec"
	"chromiumos/tast/ctxutil"
	"chromiumos/tast/errors"
	"chromiumos/tast/local/crash"
	"chromiumos/tast/local/crostini"
	"chromiumos/tast/local/dbusutil"
	"chromiumos/tast/local/vm"
	"chromiumos/tast/testing"
)

const (
	oomAnomalyEventServiceName              = "org.chromium.AnomalyEventService"
	oomAnomalyEventServicePath              = dbus.ObjectPath("/org/chromium/AnomalyEventService")
	oomAnomalyEventServiceInterface         = "org.chromium.AnomalyEventServiceInterface"
	oomAnomalyGuestFileCorruptionSignalName = "OOMEvent"
	oomEventHistogram                       = "Crostini.OOMEvent"

	killCode = 9
)

func init() {
	testing.AddTest(&testing.Test{
		Func:         OOMEvent,
		LacrosStatus: testing.LacrosVariantUnneeded,
		Desc:         "Check that OOM kill by kernel is detected correctly",
		Contacts: []string{
			// Crosvm
			"drmasquatch@google.com",
			// Telemetry
			"mutexlox@google.com",
			"cros-telemetry@google.com",
		},
		SoftwareDeps: []string{"chrome", "vm_host"},
		Attr:         []string{"group:mainline", "informational"},
		Params: []testing.Param{
			// Parameters generated by params_test.go. DO NOT EDIT.
			{
				Name:              "stable",
				ExtraSoftwareDeps: []string{"dlc"},
				ExtraHardwareDeps: crostini.CrostiniStable,
				Fixture:           "crostiniBuster",
				Timeout:           10 * time.Minute,
			}, {
				Name:              "unstable",
				ExtraAttr:         []string{"informational"},
				ExtraSoftwareDeps: []string{"dlc"},
				ExtraHardwareDeps: crostini.CrostiniUnstable,
				Fixture:           "crostiniBuster",
				Timeout:           10 * time.Minute,
			},
		},
	})
}

// OOMEvent sets up the VM and then runs a process which will be killed via
// the kernels OOM killer to check that it is detected correctly.
func OOMEvent(ctx context.Context, s *testing.State) {
	pre := s.FixtValue().(crostini.FixtureData)
	cont := pre.Cont

	// Use a shortened context for the test to reserver time for cleanup.
	cleanupCtx := ctx
	ctx, cancel := ctxutil.Shorten(ctx, 5*time.Second)
	defer cancel()

	s.Log("Setting up crash test")
	if err := crash.SetUpCrashTest(ctx, crash.WithMockConsent(), crash.DaemonStore()); err != nil {
		s.Fatal("Failed to set up crash test: ", err)
	}
	defer crash.TearDownCrashTest(ctx, crash.TearDownDaemonStore())

	if err := crash.SetUpCrashTest(ctx, crash.WithMockConsent()); err != nil {
		s.Fatal("Failed to set up crash test: ", err)
	}
	defer func() {
		if err := crash.TearDownCrashTest(cleanupCtx); err != nil {
			s.Error("Failed to tear down crash test fixture: ", err)
		}
	}()

	if err := checkDbusSignal(ctx, cont, s); err != nil {
		s.Fatal("Didn't get an error signal for OOM process: ", err)
	}

	if err := checkCrashReport(ctx, s, cont); err != nil {
		s.Fatal("Didn't find expected crash report: ", err)
	}

}

func checkDbusSignal(ctx context.Context, container *vm.Container, s *testing.State) (resultError error) {
	match := dbusutil.MatchSpec{
		Type:      "signal",
		Path:      oomAnomalyEventServicePath,
		Interface: oomAnomalyEventServiceInterface,
		Member:    oomAnomalyGuestFileCorruptionSignalName,
	}
	signalWatcher, err := dbusutil.NewSignalWatcherForSystemBus(ctx, match)
	if err != nil {
		return errors.Wrap(err, "failed to listen for DBus signals")
	}
	defer func() {
		if err := signalWatcher.Close(ctx); err != nil {
			if resultError == nil {
				resultError = err
			} else {
				testing.ContextLog(ctx, "Failed to close signal watcher: ", err)
			}
		}
	}()

	s.Log("Starting tail process")
	// tail will buffer input in-memory until it reaches a newline, so it can
	// print at least one line at a time. Since /dev/zero by definition has no
	// newlines, the memory of the process should expand until eventually the
	// kernel will be forced to kill it.
	cmd := container.VM.Command(ctx, "tail", "/dev/zero")
	code, extracted := testexec.ExitCode(cmd.Run())
	if !extracted {
		s.Fatal("Failed to extract exit code from running tail ")
	}
	if code != killCode {
		s.Fatalf("Failed to fail correctly, expected process to exit with code %v, exited with %v instead", killCode, code)
	}

	testing.ContextLog(ctx, "Waiting for signal from anomaly_detector")
	signalCtx, cancel := context.WithTimeout(ctx, 5*time.Second)
	defer cancel()
	if _, err := waitForAnomalyDetectorSignal(signalCtx, signalWatcher); err != nil {
		return errors.Wrap(err, "didn't get expected DBus signal")
	}
	testing.ContextLog(ctx, "Got expected signal from anomaly_detector")

	return nil
}

func waitForAnomalyDetectorSignal(ctx context.Context, signalWatcher *dbusutil.SignalWatcher) (*dbus.Signal, error) {
	select {
	case signal := <-signalWatcher.Signals:
		return signal, nil
	case <-ctx.Done():
		return nil, errors.New("Context deadline expired")
	}
}

func checkCrashReport(ctx context.Context, s *testing.State, container *vm.Container) error {
	testing.ContextLog(ctx, "Checking for presence of crash report")

	daemonStorePaths, err := crash.GetDaemonStoreCrashDirs(ctx)

	/*
		// Trigger a crash in the root namespace of the VM
		cmd := container.VM.Command(ctx, "bash", "-c", "kill -s SIGABRT $$")
		// Reverse the usual error checking pattern because this
		// command is supposed to crash. Instead we check that the right
		// error was encountered.
		if err := checkExitError(cmd.Run()); err != nil {
			cmd.DumpLog(ctx)
			s.Fatal("Failed to trigger crash in VM: ", err)
		}
		s.Log("Triggered a crash in the VM")
	*/

	files, err := crash.WaitForCrashFiles(ctx, daemonStorePaths,
		[]string{`.*`})
	// files, err := crash.WaitForCrashFiles(ctx, daemonStorePaths,
	_, err = crash.WaitForCrashFiles(ctx, daemonStorePaths,
		[]string{`vm_crash.*\.meta`})
	s.Log("+++++++++++++++++++++++++")
	s.Log("all the files: ", files)
	s.Log("+++++++++++++++++++++++++")
	if err != nil {
		s.Fatal("Couldn't find the expected files: ", err)
	}

	return nil
}
