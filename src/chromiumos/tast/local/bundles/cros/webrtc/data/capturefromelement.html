<!DOCTYPE html>
<html>
<body>
  <canvas id='canvas' width=10 height=10></canvas>
  <video id='video' autoplay></video>
<script src="third_party/blackframe.js"></script>
<script type="module" src="third_party/three.js/three.module.js"></script>
<script>

const WIDTH = 320;
const HEIGHT = 240;

function nextFrame(t) {
  return new Promise(resolve => {
    // We could use here requestAnimationFrame(resolve) but since the workload
    // is relatively light, we'll get such frames quite fast (~60Hz).
    setTimeout(resolve, t)
  })
}

async function drawAlternatingColours(canvas, framerate) {
  const GREEN = [0, 1, 0, 1];
  const BLUE = [0, 0, 1, 1];

  var context = canvas.getContext('webgl', {alpha : true});
  context.clearColor(GREEN[0], GREEN[1], GREEN[2], GREEN[3]);
  context.clear(context.COLOR_BUFFER_BIT);
  await nextFrame(1000 / framerate);
  context.clearColor(BLUE[0], BLUE[1], BLUE[2], BLUE[3]);
  context.clear(context.COLOR_BUFFER_BIT);
  await nextFrame(1000 / framerate);

  drawAlternatingColours(canvas, framerate);
}

async function renderGetUserMediaWithPerspective(canvas, width, height) {
  const constraints = {
    audio: false,
    video: {
      width: width,
      height: height
    }
  };
  const stream = await navigator.mediaDevices.getUserMedia(constraints);
  video = document.createElement('video');
  video.autoplay = true;
  video.srcObject = stream;

  let THREE = await import ('/third_party/three.js/three.module.js');

  let camera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);
  let scene = new THREE.Scene();

  const texture = new THREE.VideoTexture(video);
  texture.needsUpdate;
  texture.minFilter = THREE.LinearFilter;
  texture.magFilter = THREE.LinearFilter;
  texture.format = THREE.RGBFormat;
  texture.crossOrigin = 'anonymous';

  const material = new THREE.MeshBasicMaterial({
    map: texture
  });

  const geometry = new THREE.PlaneGeometry(width / height, 1);
  geometry.rotateX(-Math.PI / 12); // pi / 12 = 15 deg.

  const mesh = new THREE.Mesh(geometry, material);
  mesh.position.z = -1;
  scene.add(mesh);

  let renderer = new THREE.WebGLRenderer({
    antialias: true,
    canvas: canvas
  });
  renderer.setPixelRatio(width / height);
  renderer.setSize(width, height);

  animate(scene, camera, renderer);
}

function animate(scene, camera, renderer) {
  renderer.render(scene, camera);
  requestAnimationFrame((timestamp) => {
    animate(scene, camera, renderer)
  });
}

async function captureCanvasAndInspect1(validate) {
  const canvas = document.getElementById('canvas');
  const FRAMERATE = 24;
  canvas.width = WIDTH;
  canvas.height = HEIGHT;
  await drawAlternatingColours(canvas, FRAMERATE);
  await captureCanvasAndInspect(validate)
}
async function captureCanvasAndInspect2(validate) {
  const canvas = document.getElementById('canvas');
  await renderGetUserMediaWithPerspective(canvas, WIDTH, HEIGHT);
  await captureCanvasAndInspect(validate)
}

async function captureCanvasAndInspect(validate) {
  const canvas = document.getElementById('canvas');

  // Note that no parameters to captureStream() will force a new capture every
  // time |canvas| is modified.
  var stream = canvas.captureStream();
  if (stream.getVideoTracks().length !== 1) {
    throw new DOMException('Wrong getVideoTracks() length',
        'InvalidStateError');
  }

  const video = document.getElementById('video');
  video.srcObject = stream;
  await video.play();

  if (!validate) {
    return;
  }

  const mediaStreamSettings = stream.getVideoTracks()[0].getSettings();
  video.width = mediaStreamSettings.width;
  video.height = mediaStreamSettings.height;

  if (isNaN(video.width) || isNaN(video.height) || video.width === 0 ||
      video.height === 0) {
    throw new DOMException(`Bad stream dimensions ` +
                            `${video.width}x${video.height}`);
  }
  if (video.width !== WIDTH || video.height !== HEIGHT) {
    throw new DOMException(`Unexpected capture resolution, got: `+
      `${video.width}x${video.height}, expected ` + `${WIDTH}x${HEIGHT}`);
  }

  // Draw the <video> contents in an offscreen canvas to validate the pixels.
  const width = Math.max(video.width / 8, 128);
  const height = Math.max(video.height / 8, 128);
  const context = new OffscreenCanvas(width, height).getContext('2d');
  context.drawImage(video, 0, 0, width, height);
  const imageData = context.getImageData(0, 0, width, height);
  if (isBlackFrame(imageData.data, imageData.data.length)) {
    throw new DOMException('Captured data is considered all black');
  }
}

</script>
</body>
</html>
